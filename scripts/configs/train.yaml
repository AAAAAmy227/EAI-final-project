# Default configuration for PPO training
defaults:
  - env: track1
  - reward: lift
  - control: single_arm  # Use dual_arm for sort task
  - obs: single_arm      # Use dual_arm for sort task
  - _self_

# Experiment settings
exp_name: "${env.task}"
seed: 1
device_id: 0  # GPU device ID for simulation and rendering (e.g., 0, 1, 2...)

# Logging
wandb:
  enabled: true
  project: "Track1-PPO-lift"
  entity: null

# Optimization flags
compile: true
cudagraphs: true
anneal_lr: true

# Training hyperparameters
training:
  total_timesteps: 1_000_000_000
  num_envs: 2048
  num_eval_envs: 1
  num_steps: 32
  num_eval_steps: null
  eval_step_multiplier: 1.0
  eval_freq: 200000  # Run eval every 200k steps (reasonable for long training)
  skip_eval: false  # Must be false to enable video generation

# PPO hyperparameters
ppo:
  learning_rate: 1e-4
  gamma: 0.99
  gae_lambda: 0.95
  num_minibatches: 16
  update_epochs: 5
  clip_coef: 0.2
  clip_vloss: false      # Whether to clip value loss (as per original PPO paper)
  norm_adv: true        # Whether to normalize advantages per minibatch
  ent_coef: 0.001
  vf_coef: 0.5
  max_grad_norm: 0.5
  target_kl: 0.015      # Reduced from 0.2 to prevent policy collapse (Standard PPO: 0.01-0.02)
  reward_scale: 1.0
  handle_timeout_termination: true  # If true, bootstrap on truncation (recommended)
  logstd_init: -1.0      # Initial log standard deviation (e.g. 0.0 = std 1.0, -0.5 = std 0.6)
  logstd_min: -10.0       # Minimum log standard deviation clamp
  logstd_max: 0.0        # Maximum log standard deviation clamp

# Model saving
save_model: true
capture_video: true
checkpoint: null

# Debugging/Monitoring
log_obs_stats: true  # Log mean/std of observation inputs (useful for monitoring normalization)
async_eval: false     # Run evaluation in background thread (non-blocking)

# Observation Normalization (like gym.wrappers.NormalizeObservation, but GPU-compatible)
normalize_obs: true    # Whether to apply running normalization to observations (Welford's online algorithm)
obs_norm_epsilon: 1e-8 # Numerical stability epsilon for observation normalization
obs_clip: 10.0         # Clip normalized observations to [-clip, clip]

# Reward Normalization (like gym.wrappers.NormalizeReward, but GPU-compatible)
normalize_reward: true       # Whether to apply running reward normalization
reward_norm_gamma: $(ppo.gamma)      # Gamma for discounted return tracking (should match ppo.gamma)
reward_norm_epsilon: 1e-8    # Numerical stability epsilon for reward normalization
reward_clip: 10.0            # Clip normalized rewards to [-clip, clip]
# Value Normalization (PopArt)
# Use this when you have mixed dense/sparse rewards with different scales
# PopArt normalizes value targets while preserving critic outputs
value_normalization: false
popart_beta: 0.0001  # Update rate for running stats (smaller = more stable)

# Recording Configuration (for evaluation)
recording:
  save_video: true           # Enable video recording during eval
  save_trajectory: true      # Enable .h5 trajectory recording
  save_env_state: false      # Record environment state in trajectory (increases file size)
  save_step_csv: true        # Export per-step reward data to CSV for analysis
  info_on_video: true        # Overlay reward/success info on video frames
  video_fps: 30              # FPS for recorded videos
