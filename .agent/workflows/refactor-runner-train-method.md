---
description: é‡æ„ PPORunner.train() æ–¹æ³• - æå–å­æ–¹æ³•é™ä½å¤æ‚åº¦
---

# ä»»åŠ¡ï¼šPPORunner.train() å­æ–¹æ³•æå–

## ğŸ“‹ ä»»åŠ¡æ¦‚è¿°

`scripts/training/runner.py` ä¸­çš„ `train()` æ–¹æ³•çº¦ 250 è¡Œï¼ŒèŒè´£è¿‡å¤šã€‚éœ€è¦æå–å­æ–¹æ³•æ¥é™ä½å¤æ‚åº¦ï¼Œæé«˜å¯è¯»æ€§ã€‚

## ğŸ¯ ç›®æ ‡

1. å°† `train()` æ–¹æ³•ä» ~250 è¡Œå‡å°‘åˆ° ~80 è¡Œ
2. æå– 4-5 ä¸ªèŒè´£å•ä¸€çš„ç§æœ‰æ–¹æ³•
3. ä¿æŒåŠŸèƒ½å®Œå…¨ä¸å˜

---

## ğŸ“ å½“å‰ train() ç»“æ„åˆ†æ

```python
def train(self):
    # ç¬¬ 436-447 è¡Œ: åˆå§‹åŒ–å’Œé¦–æ¬¡ reset
    # ç¬¬ 448-451 è¡Œ: è®­ç»ƒå¾ªç¯è®¾ç½®
    
    for iteration in pbar:
        # ç¬¬ 455-457 è¡Œ: burnin é€»è¾‘
        # ç¬¬ 460-466 è¡Œ: å­¦ä¹ ç‡è°ƒåº¦
        # ç¬¬ 468-469 è¡Œ: cudagraph æ ‡è®°
        
        # ç¬¬ 471-473 è¡Œ: Rollout
        # ç¬¬ 475-491 è¡Œ: GAE è®¡ç®—
        # ç¬¬ 493-518 è¡Œ: PPO Update
        
        # ç¬¬ 520-639 è¡Œ: æ—¥å¿—è®°å½• (120+ è¡Œ!)
        
        # ç¬¬ 641-672 è¡Œ: è¯„ä¼°è°ƒåº¦
    
    # ç¬¬ 674-681 è¡Œ: æ¸…ç†
```

---

## ğŸ“ é‡æ„æ–¹æ¡ˆ

### æå–çš„å­æ–¹æ³•

| æ–°æ–¹æ³•å | åŸä½ç½® | èŒè´£ |
|---------|-------|------|
| `_log_training_metrics()` | 520-639 | æ„å»ºå¹¶è®°å½•æ—¥å¿— |
| `_run_ppo_update()` | 493-518 | PPO æ›´æ–°å¾ªç¯ |
| `_compute_gae()` | 475-491 | GAE è®¡ç®— |
| `_schedule_learning_rate()` | 462-466 | å­¦ä¹ ç‡è°ƒåº¦ |
| `_handle_evaluation()` | 644-672 | è¯„ä¼°è°ƒåº¦ |

---

## ğŸ“ éœ€è¦ä¿®æ”¹çš„ä»£ç 

### æ–‡ä»¶: `scripts/training/runner.py`

#### æ–°å¢æ–¹æ³• 1: `_schedule_learning_rate()` 

**åœ¨ `_rollout()` æ–¹æ³•ä¹‹åæ·»åŠ :**

```python
def _schedule_learning_rate(self, iteration: int):
    """Anneal learning rate linearly over training."""
    if self.anneal_lr:
        frac = 1.0 - (iteration - 1.0) / self.num_iterations
        lrnow = frac * self.cfg.ppo.learning_rate
        self.optimizer.param_groups[0]["lr"].copy_(lrnow)
```

---

#### æ–°å¢æ–¹æ³• 2: `_compute_gae()`

```python
def _compute_gae(self, container, next_obs, next_bootstrap_mask):
    """Compute GAE advantages and returns.
    
    Args:
        container: TensorDict with rollout data (rewards, vals, bootstrap_mask)
        next_obs: Final observations after rollout
        next_bootstrap_mask: Bootstrap mask for final step
        
    Returns:
        Updated container with 'advantages' and 'returns' added
    """
    with torch.no_grad():
        # next_obs is already normalized by wrapper
        next_value = self.get_value(next_obs)
    
    advs, rets = self.gae_fn(
        container["rewards"],
        container["vals"],
        container["bootstrap_mask"],
        next_value,
        next_bootstrap_mask
    )
    container["advantages"] = advs
    container["returns"] = rets
    return container
```

---

#### æ–°å¢æ–¹æ³• 3: `_run_ppo_update()`

```python
def _run_ppo_update(self, container):
    """Run PPO update epochs.
    
    Args:
        container: TensorDict with obs, actions, logprobs, advantages, returns
        
    Returns:
        dict with final update metrics (v_loss, pg_loss, entropy_loss, etc.)
        list of clipfracs from all minibatches
    """
    container_flat = container.view(-1)
    clipfracs = []
    
    for epoch in range(self.cfg.ppo.update_epochs):
        b_inds = torch.randperm(container_flat.shape[0], device=self.device).split(self.minibatch_size)
        for b in b_inds:
            container_local = container_flat[b]
            out = self.update_fn(container_local, tensordict_out=tensordict.TensorDict())
            clipfracs.append(out["clipfrac"].item())
            
            if self.cfg.ppo.target_kl is not None and out["approx_kl"] > self.cfg.ppo.target_kl:
                break
        else:
            continue
        break
    
    return out, clipfracs
```

---

#### æ–°å¢æ–¹æ³• 4: `_log_training_metrics()` 

è¿™æ˜¯æœ€å¤§çš„æå–ï¼Œçº¦ 120 è¡Œã€‚

```python
def _log_training_metrics(self, iteration: int, container, out, clipfracs, training_time: float):
    """Build and log training metrics to console and WandB.
    
    Args:
        iteration: Current training iteration
        container: TensorDict with rollout data
        out: Dict with final PPO update metrics
        clipfracs: List of clip fractions from all minibatches
        training_time: Accumulated training time in seconds
    """
    if training_time <= 0:
        return {}
    
    speed = (self.global_step - self.global_step_burnin) / training_time
    avg_return = np.mean(self.avg_returns) if self.avg_returns else 0
    lr = self.optimizer.param_groups[0]["lr"]
    if isinstance(lr, torch.Tensor):
        lr = lr.item()
    
    # Explained variance
    y_pred = container["vals"].flatten().cpu().numpy()
    y_true = container["returns"].flatten().cpu().numpy()
    var_y = np.var(y_true)
    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y
    
    # Build base logs
    logs = {
        "charts/SPS": speed,
        "charts/learning_rate": lr,
        "charts/terminated_count": self.terminated_count,
        "charts/truncated_count": self.truncated_count,
        "losses/value_loss": out["v_loss"].item(),
        "losses/policy_loss": out["pg_loss"].item(),
        "losses/entropy": out["entropy_loss"].item(),
        "losses/approx_kl": out["approx_kl"].item(),
        "losses/old_approx_kl": out["old_approx_kl"].item(),
        "losses/clipfrac": np.mean(clipfracs),
        "losses/explained_variance": explained_var,
        "losses/grad_norm": out["gn"].item() if isinstance(out["gn"], torch.Tensor) else out["gn"],
        "rollout/ep_return_mean": avg_return,
        "rollout/rewards_mean": container["rewards"].mean().item(),
        "rollout/rewards_max": container["rewards"].max().item(),
    }
    
    # Add reward components
    logs.update(self._build_reward_component_logs())
    
    # Add observation stats if enabled
    if self.log_obs_stats:
        logs.update(self._build_obs_stats_logs(container))
    
    # Log to WandB
    if self.cfg.wandb.enabled:
        wandb.log(logs, step=self.global_step)
    
    return logs
```

---

#### æ–°å¢è¾…åŠ©æ–¹æ³• 5: `_build_reward_component_logs()` 

```python
def _build_reward_component_logs(self) -> dict:
    """Build reward component logs and reset accumulators."""
    logs = {}
    
    if self.reward_component_count > 0:
        for name, total in self.reward_component_sum.items():
            logs[f"reward/{name}"] = total / self.reward_component_count
        logs["reward/success_count"] = self.success_count.item() if hasattr(self.success_count, 'item') else self.success_count
        logs["reward/fail_count"] = self.fail_count.item() if hasattr(self.fail_count, 'item') else self.fail_count
        
        # Reset accumulators
        self.reward_component_sum = {}
        self.reward_component_count = 0
        self.success_count = torch.tensor(0, device=self.device, dtype=torch.float32)
        self.fail_count = torch.tensor(0, device=self.device, dtype=torch.float32)
    
    return logs
```

---

#### æ–°å¢è¾…åŠ©æ–¹æ³• 6: `_build_obs_stats_logs()`

```python
def _build_obs_stats_logs(self, container) -> dict:
    """Build observation statistics logs."""
    from scripts.training.env_utils import NormalizeObservationGPU, NormalizeRewardGPU, find_wrapper
    
    logs = {}
    
    # Raw observation stats
    raw_obs = container["obs"]
    raw_mean = raw_obs.mean(dim=(0, 1))
    raw_std = raw_obs.std(dim=(0, 1))
    logs["obs_raw/mean_avg"] = raw_mean.mean().item()
    logs["obs_raw/std_avg"] = raw_std.mean().item()
    logs["obs_raw/min"] = raw_obs.min().item()
    logs["obs_raw/max"] = raw_obs.max().item()
    
    # Obs wrapper stats
    obs_wrapper = find_wrapper(self.envs, NormalizeObservationGPU)
    if obs_wrapper is not None:
        obs_rms = obs_wrapper.rms
        obs_rms_std = torch.sqrt(obs_rms.var + 1e-8)
        for i, name in enumerate(self.obs_names):
            if i < len(obs_rms.mean):
                logs[f"obs_rms_mean/{name}"] = obs_rms.mean[i].item()
                logs[f"obs_rms_std/{name}"] = obs_rms_std[i].item()
    
    # Reward wrapper stats
    reward_wrapper = find_wrapper(self.envs, NormalizeRewardGPU)
    if reward_wrapper is not None:
        r_rms = reward_wrapper.rms
        logs["reward_norm/return_rms_var"] = r_rms.var.item()
        logs["reward_norm/return_rms_std"] = torch.sqrt(r_rms.var + 1e-8).item()
        logs["reward_norm/return_rms_mean"] = r_rms.mean.item()
        logs["reward_norm/return_rms_count"] = r_rms.count if isinstance(r_rms.count, (int, float)) else r_rms.count.item()
        logs["reward_norm/normalized_reward_mean"] = container["rewards"].mean().item()
        logs["reward_norm/normalized_reward_std"] = container["rewards"].std().item()
    
    return logs
```

---

#### æ–°å¢æ–¹æ³• 7: `_handle_evaluation()`

```python
def _handle_evaluation(self, iteration: int):
    """Handle evaluation scheduling (sync or async)."""
    if self.async_eval:
        # Async eval: launch in background
        if self.eval_thread is not None and self.eval_thread.is_alive():
            self.eval_thread.join()
        
        self.eval_agent.load_state_dict(self.agent.state_dict())
        self.eval_thread = threading.Thread(
            target=self._evaluate_async,
            args=(iteration,),
            daemon=True
        )
        self.eval_thread.start()
        print(f"  [Async] Eval launched in background (iteration {iteration})")
    else:
        # Sync eval (blocking)
        eval_start = time.time()
        self._evaluate()
        eval_duration = time.time() - eval_start
        print(f"  Eval took {eval_duration:.2f}s")
        if self.cfg.wandb.enabled:
            wandb.log({"charts/eval_time": eval_duration}, step=self.global_step)
        self._save_checkpoint(iteration)
```

---

#### é‡æ„åçš„ `train()` æ–¹æ³•

**åŸæ¥ ~250 è¡Œï¼Œé‡æ„å ~80 è¡Œ:**

```python
def train(self):
    print(f"\n{'='*60}")
    print(f"Training PPO (LeanRL-style) on Track1 {self.cfg.env.task}")
    print(f"Device: {self.device}, Compile: {self.compile}, CudaGraphs: {self.cudagraphs}")
    print(f"Total Timesteps: {self.total_timesteps}, Batch Size: {self.batch_size}")
    print(f"{'='*60}\n")
    
    # Initial reset
    next_obs, _ = self.envs.reset(seed=self.cfg.seed)
    next_bootstrap_mask = torch.zeros(self.num_envs, device=self.device, dtype=torch.bool)
    
    pbar = tqdm.tqdm(range(1, self.num_iterations + 1))
    self.global_step_burnin = None
    training_time = 0.0
    measure_burnin = 2
    
    for iteration in pbar:
        if iteration == measure_burnin:
            self.global_step_burnin = self.global_step
            training_time = 0.0
        
        iter_start_time = time.time()
        
        # Learning rate annealing
        self._schedule_learning_rate(iteration)
        
        # Cudagraph marker
        torch.compiler.cudagraph_mark_step_begin()
        
        # Rollout
        next_obs, next_bootstrap_mask, container = self._rollout(next_obs, next_bootstrap_mask)
        self.global_step += container.numel()
        
        # GAE calculation
        container = self._compute_gae(container, next_obs, next_bootstrap_mask)
        
        # PPO update
        out, clipfracs = self._run_ppo_update(container)
        
        # Sync params to inference agent (CudaGraph only)
        if self.cudagraphs:
            from_module(self.agent).data.to_module(self.agent_inference)
        
        # Logging
        if self.global_step_burnin is not None and training_time > 0:
            self._log_training_metrics(iteration, container, out, clipfracs, training_time)
            avg_return = np.mean(self.avg_returns) if self.avg_returns else 0
            pbar.set_description(f"SPS: {(self.global_step - self.global_step_burnin) / training_time:.0f}, return: {avg_return:.2f}")
        
        # Accumulate training time
        training_time += time.time() - iter_start_time
        
        # Evaluation
        if iteration % self.cfg.training.eval_freq == 0:
            self._handle_evaluation(iteration)
    
    # Cleanup
    if self.async_eval and self.eval_thread is not None and self.eval_thread.is_alive():
        print("Waiting for async eval to complete...")
        self.eval_thread.join()
    
    self.envs.close()
    self.eval_envs.close()
    print("Training complete!")
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **å®ä¾‹å˜é‡**: æ–°æ–¹æ³•éœ€è¦è®¿é—® `self.global_step_burnin`ï¼Œéœ€è¦åœ¨ `__init__` ä¸­åˆå§‹åŒ–ä¸º `None`
2. **Import ä½ç½®**: `find_wrapper` import åº”æ”¾åœ¨éœ€è¦çš„æ–¹æ³•å†…éƒ¨ (å»¶è¿Ÿå¯¼å…¥)
3. **æ–¹æ³•é¡ºåº**: æ–°æ–¹æ³•åº”æ”¾åœ¨ `train()` ä¹‹åï¼Œ`_evaluate()` ä¹‹å‰
4. **Progress bar**: ä¿æŒ pbar.set_description æ›´æ–°

---

## âœ… éªŒæ”¶æ ‡å‡†

1. **è¯­æ³•æ­£ç¡®**:
   ```bash
   uv run python -m py_compile scripts/training/runner.py
   ```

2. **è¡Œæ•°å‡å°‘**: `train()` æ–¹æ³•ä» ~250 è¡Œå‡å°‘åˆ° ~80 è¡Œ

3. **åŠŸèƒ½ä¸å˜**: è®­ç»ƒ/è¯„ä¼°/æ—¥å¿—è¡Œä¸ºä¸é‡æ„å‰ä¸€è‡´

---

## ğŸ“ ç›¸å…³æ–‡ä»¶è·¯å¾„

- `/home/admin/Desktop/eai-final-project/scripts/training/runner.py`

## ğŸ”— å‰ç½®ä¾èµ–

- `/refactor-wrapper-traversal` å¿…é¡»å·²å®Œæˆ (æä¾› `find_wrapper`)
- `/fix-runner-missing-methods` å¿…é¡»å·²å®Œæˆ (ä¿®å¤æœªå®šä¹‰æ–¹æ³•)

---

// turbo-all
