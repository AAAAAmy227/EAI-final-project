# EAI Final Project - Robotic Manipulation with PPO

å¼ºåŒ–å­¦ä¹ è®­ç»ƒæœºå™¨äººæ“ä½œä»»åŠ¡ï¼ˆåŸºäº ManiSkill å’Œ PPO ç®—æ³•ï¼‰

## é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®ä½¿ç”¨ PPO (Proximal Policy Optimization) ç®—æ³•è®­ç»ƒåŒè‡‚æœºå™¨äººï¼ˆSO-101ï¼‰å®Œæˆå„ç§æ“ä½œä»»åŠ¡ã€‚æ ¸å¿ƒç‰¹æ€§åŒ…æ‹¬ï¼š

- ğŸ¤– **åŒè‡‚æœºå™¨äºº**: SO-101 åŒè‡‚åä½œæ§åˆ¶
- ğŸ¯ **å¤šä»»åŠ¡æ”¯æŒ**: Lift, Stack, Sort ç­‰ä»»åŠ¡
- âš¡ **GPU åŠ é€Ÿ**: å¹¶è¡Œç¯å¢ƒæ¨¡æ‹Ÿå’Œè®­ç»ƒ
- ğŸ“Š **ç»Ÿä¸€ Metrics ç³»ç»Ÿ**: è®­ç»ƒå’Œè¯„ä¼°çš„æŒ‡æ ‡æ”¶é›†
- ğŸ”„ **å¼‚æ­¥è¯„ä¼°**: åå°è¯„ä¼°ä¸å½±å“è®­ç»ƒé€Ÿåº¦

## ç¯å¢ƒè¦æ±‚

- **Python**: 3.10+
- **åŒ…ç®¡ç†**: uv
- **GPU**: CUDA-capable GPU (æ¨è)
- **ManiSkill**: å·²å®‰è£…åœ¨ `.venv`

## å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–

```bash
# ä½¿ç”¨ uv åˆ›å»ºè™šæ‹Ÿç¯å¢ƒå¹¶å®‰è£…ä¾èµ–
uv sync
```

### è®­ç»ƒ

```bash
# åŸºç¡€è®­ç»ƒï¼ˆLift ä»»åŠ¡ï¼‰
uv run python scripts/train.py

# æŒ‡å®šä»»åŠ¡
uv run python scripts/train.py env.task=stack

# è‡ªå®šä¹‰é…ç½®
uv run python scripts/train.py \
    training.num_envs=512 \
    ppo.learning_rate=3e-4 \
    wandb.enabled=true
```

### è¯„ä¼°

```bash
# è¯„ä¼°å·²è®­ç»ƒçš„æ¨¡å‹
uv run python scripts/train.py \
    checkpoint=path/to/checkpoint.pth \
    training.num_eval_envs=16 \
    capture_video=true
```

## é¡¹ç›®ç»“æ„

```
eai-final-project/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ training/              # è®­ç»ƒç›¸å…³æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ runner.py          # PPO Runnerï¼ˆæ ¸å¿ƒè®­ç»ƒå¾ªç¯ï¼‰
â”‚   â”‚   â”œâ”€â”€ agent.py           # Actor-Critic ç½‘ç»œ
â”‚   â”‚   â”œâ”€â”€ ppo_utils.py       # PPO ç®—æ³•å®ç°ï¼ˆGAE, æ›´æ–°ï¼‰
â”‚   â”‚   â”œâ”€â”€ env_utils.py       # ç¯å¢ƒå·¥å…·ï¼ˆwrappers, make_envï¼‰
â”‚   â”‚   â””â”€â”€ metrics_utils.py   # Metrics æ”¶é›†å’Œèšåˆ
â”‚   â”‚
â”‚   â”œâ”€â”€ tasks/                 # ä»»åŠ¡å®šä¹‰
â”‚   â”‚   â”œâ”€â”€ base.py            # BaseTaskHandlerï¼ˆåŸºç±»ï¼‰
â”‚   â”‚   â”œâ”€â”€ lift.py            # LiftTaskHandler
â”‚   â”‚   â”œâ”€â”€ stack.py           # StackTaskHandler
â”‚   â”‚   â””â”€â”€ sort.py            # SortTaskHandler
â”‚   â”‚
â”‚   â”œâ”€â”€ envs/                  # ç¯å¢ƒå®šä¹‰
â”‚   â”‚   â””â”€â”€ track1_env.py      # Track1Envï¼ˆä¸»ç¯å¢ƒï¼‰
â”‚   â”‚
â”‚   â”œâ”€â”€ tests/                 # å•å…ƒæµ‹è¯•
â”‚   â”‚   â”œâ”€â”€ test_metrics.py        # Metrics ç³»ç»Ÿæµ‹è¯•
â”‚   â”‚   â”œâ”€â”€ test_task_handlers.py  # TaskHandler æµ‹è¯•
â”‚   â”‚   â”œâ”€â”€ test_ppo_unit.py       # PPO å•å…ƒæµ‹è¯•
â”‚   â”‚   â”œâ”€â”€ test_ppo_integration.py # PPO é›†æˆæµ‹è¯•
â”‚   â”‚   â””â”€â”€ README.md              # æµ‹è¯•æ–‡æ¡£
â”‚   â”‚
â”‚   â”œâ”€â”€ so101.py               # SO-101 åŒè‡‚æœºå™¨äººå®šä¹‰
â”‚   â”œâ”€â”€ train.py               # è®­ç»ƒå…¥å£
â”‚   â”œâ”€â”€ view_env.py            # ç¯å¢ƒå¯è§†åŒ–
â”‚   â”‚
â”‚   â”œâ”€â”€ benchmarks/            # æ€§èƒ½æµ‹è¯•
â”‚   â”‚   â”œâ”€â”€ benchmark_full_loop.py
â”‚   â”‚   â”œâ”€â”€ benchmark_gae.py
â”‚   â”‚   â””â”€â”€ benchmark_ppo.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/                 # å·¥å…·è„šæœ¬
â”‚       â”œâ”€â”€ camera_overlay.py
â”‚       â”œâ”€â”€ check_wrist_camera.py
â”‚       â”œâ”€â”€ sample_poses_ik.py
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ configs/                   # Hydra é…ç½®æ–‡ä»¶
â”‚   â””â”€â”€ train.yaml            # é»˜è®¤è®­ç»ƒé…ç½®
â”‚
â”œâ”€â”€ outputs/                   # è®­ç»ƒè¾“å‡ºï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰
â”‚   â””â”€â”€ YYYY-MM-DD/
â”‚       â””â”€â”€ HH-MM-SS/
â”‚           â”œâ”€â”€ checkpoints/   # æ¨¡å‹æ£€æŸ¥ç‚¹
â”‚           â”œâ”€â”€ videos/        # è¯„ä¼°è§†é¢‘
â”‚           â”œâ”€â”€ split/         # åˆ†ç¯å¢ƒè§†é¢‘å’ŒCSV
â”‚           â””â”€â”€ .hydra/        # Hydra é…ç½®å¿«ç…§
â”‚
â”œâ”€â”€ assets/                    # èµ„æºæ–‡ä»¶
â”‚   â””â”€â”€ screenshots/
â”‚
â””â”€â”€ README.md                  # æœ¬æ–‡ä»¶
```

## æ ¸å¿ƒæ¨¡å—è¯´æ˜

### 1. Training Pipeline (`scripts/training/`)

#### `runner.py` - PPO Runner
æ ¸å¿ƒè®­ç»ƒå¾ªç¯ï¼Œè´Ÿè´£ï¼š
- ç¯å¢ƒäº¤äº’ï¼ˆrolloutï¼‰
- GAE è®¡ç®—
- PPO æ›´æ–°
- è¯„ä¼°è°ƒåº¦
- æŒ‡æ ‡è®°å½•

**å…³é”®æ–¹æ³•**ï¼š
- `_rollout()`: ç»Ÿä¸€çš„ rollout æ–¹æ³•ï¼ˆæ”¯æŒ train å’Œ evalï¼‰
- `_compute_gae()`: GAE ä¼˜åŠ¿ä¼°è®¡
- `_run_ppo_update()`: PPO å‚æ•°æ›´æ–°
- `_evaluate()`: è¯„ä¼°å¾ªç¯

#### `metrics_utils.py` - Metrics ç³»ç»Ÿ
ç»Ÿä¸€çš„æŒ‡æ ‡æ”¶é›†å’Œèšåˆç³»ç»Ÿï¼š
- `get_metric_specs_from_env()`: ä» TaskHandler è·å– metric specs
- `aggregate_metrics()`: æ‰¹é‡èšåˆ rollout çš„ metrics

**ç‰¹æ€§**ï¼š
- âœ… GPU æ‰¹é‡æ“ä½œ
- âœ… å»¶è¿Ÿ CPU ä¼ è¾“
- âœ… Mode-specific metricsï¼ˆtrain vs evalï¼‰
- âœ… è‡ªåŠ¨èšåˆï¼ˆmean / sumï¼‰

### 2. Task System (`scripts/tasks/`)

#### `base.py` - BaseTaskHandler
ä»»åŠ¡å¤„ç†å™¨åŸºç±»ï¼Œå®šä¹‰æ¥å£ï¼š
- `evaluate()`: è¯„ä¼°æˆåŠŸ/å¤±è´¥æ¡ä»¶
- `compute_dense_reward()`: è®¡ç®—å¯†é›†å¥–åŠ±
- `initialize_episode()`: åˆå§‹åŒ– episode

**Metrics å®šä¹‰**ï¼š
```python
class BaseTaskHandler:
    # é»˜è®¤ metricsï¼ˆæ‰€æœ‰ä»»åŠ¡å…±äº«ï¼‰
    DEFAULT_METRIC_AGGREGATIONS = {
        "success": "mean",
        "fail": "mean",
        "raw_reward": "mean",
        "return": "mean",
        "episode_len": "mean",
    }
    
    @classmethod
    def _get_train_metrics(cls) -> Dict[str, str]:
        """å®šä¹‰ training ä¸“ç”¨ metrics"""
        return {}
    
    @classmethod
    def _get_eval_metrics(cls) -> Dict[str, str]:
        """å®šä¹‰ evaluation ä¸“ç”¨ metricsï¼ˆé»˜è®¤ä¸ train ç›¸åŒï¼‰"""
        return cls._get_train_metrics()
```

#### `lift.py` - LiftTaskHandler
Lift ä»»åŠ¡å®ç°ï¼ˆæŠ“å–å¹¶ä¸¾èµ·ç‰©ä½“ï¼‰

**è‡ªå®šä¹‰ Metrics**ï¼š
```python
class LiftTaskHandler(BaseTaskHandler):
    @classmethod
    def _get_train_metrics(cls):
        return {
            "grasp_reward": "mean",
            "lift_reward": "mean",
            "moving_distance": "mean",
            "grasp_success": "mean",
            "lift_success": "mean",
        }
```

### 3. Environment (`scripts/envs/`)

#### `track1_env.py` - Track1Env
ä¸»ç¯å¢ƒç±»ï¼Œç»§æ‰¿è‡ª ManiSkill çš„ `BaseEnv`ï¼š
- åœºæ™¯è®¾ç½®ï¼ˆrobot, objects, camerasï¼‰
- è§‚æµ‹ç©ºé—´å®šä¹‰
- åŠ¨ä½œç©ºé—´å®šä¹‰
- å¥–åŠ±è®¡ç®—ï¼ˆå§”æ‰˜ç»™ TaskHandlerï¼‰

### 4. Robot (`scripts/so101.py`)

SO-101 åŒè‡‚æœºå™¨äººå®šä¹‰ï¼š
- URDF åŠ è½½
- æ§åˆ¶å™¨é…ç½®
- è¿åŠ¨å­¦å‚æ•°

## Metrics ç³»ç»Ÿè¯¦è§£

### Mode-Specific Metrics

æ”¯æŒä¸º training å’Œ evaluation å®šä¹‰ä¸åŒçš„ metricsï¼š

```python
class DetailedTaskHandler(BaseTaskHandler):
    @classmethod
    def _get_train_metrics(cls):
        """Training: åªæ”¶é›†å…³é”® metricsï¼ˆæ€§èƒ½ä¼˜å…ˆï¼‰"""
        return {
            "grasp_reward": "mean",
            "lift_reward": "mean",
        }
    
    @classmethod
    def _get_eval_metrics(cls):
        """Evaluation: æ”¶é›†è¯¦ç»† metricsï¼ˆåˆ†æä¼˜å…ˆï¼‰"""
        return {
            "grasp_reward": "mean",
            "lift_reward": "mean",
            "cube_velocity": "mean",      # Eval-only
            "gripper_distance": "mean",   # Eval-only
            "stability_score": "mean",    # Eval-only
        }
```

### è‡ªåŠ¨æ¨¡å¼åˆ‡æ¢

åœ¨ `_rollout()` ä¸­è‡ªåŠ¨æ ¹æ® `collect_for_training` å‚æ•°é€‰æ‹©ï¼š
- `collect_for_training=True` â†’ `mode="train"`
- `collect_for_training=False` â†’ `mode="eval"`

### èšåˆç±»å‹

- **`"mean"`**: è®¡ç®—å¹³å‡å€¼ï¼ˆé€‚ç”¨äº rewards, success rateï¼‰
- **`"sum"`**: ç´¯åŠ æ€»å’Œï¼ˆé€‚ç”¨äº countsï¼‰

### Logging

**Training Logs**:
```python
wandb.log({
    "rollout/success_rate": 0.75,
    "rollout/return": 10.5,
    "reward/grasp_reward": 2.3,
    "reward/lift_reward": 8.2,
}, step=10240)
```

**Evaluation Logs**:
```python
wandb.log({
    "eval/success_rate": 0.82,
    "eval/return": 12.1,
    "eval_reward/grasp_reward": 2.5,
    "eval_reward/lift_reward": 9.6,
    "eval_reward/cube_velocity": 0.15,  # Eval-only
}, step=10240)
```

## é…ç½®ç³»ç»Ÿ

ä½¿ç”¨ Hydra è¿›è¡Œé…ç½®ç®¡ç†ï¼ˆ`configs/train.yaml`ï¼‰ï¼š

```yaml
# ç¯å¢ƒé…ç½®
env:
  task: lift
  num_envs: 256
  robot_urdf: assets/so101.urdf

# PPO é…ç½®
ppo:
  learning_rate: 3e-4
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  
# Training é…ç½®
training:
  total_timesteps: 10_000_000
  num_steps: 2048
  num_eval_envs: 16
  eval_freq: 10

# WandB é…ç½®
wandb:
  enabled: true
  project: eai-final-project
```

## æ€§èƒ½ä¼˜åŒ–

### GPU ä¼˜åŒ–
- âœ… æ‰€æœ‰ metrics åœ¨ GPU ä¸Šæ”¶é›†
- âœ… å»¶è¿Ÿ CPU ä¼ è¾“ï¼ˆrollout ç»“æŸåæ‰¹é‡ï¼‰
- âœ… å‘é‡åŒ–æ“ä½œ

### å¼‚æ­¥è¯„ä¼°
- âœ… åå°çº¿ç¨‹è¿è¡Œ evaluation
- âœ… ç‹¬ç«‹çš„ CUDA stream
- âœ… ä¸å½±å“ training é€Ÿåº¦
- âœ… å‡†ç¡®çš„ step logging

### ç¼–è¯‘å’Œ CUDA Graphs
- âœ… `torch.compile` åŠ é€Ÿ
- âœ… CudaGraphModule ç”¨äº policy inference
- âœ… Reduce-overhead mode ç”¨äº update

## æµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
uv run pytest scripts/tests/ -v

# è¿è¡Œç‰¹å®šæµ‹è¯•
uv run pytest scripts/tests/test_metrics.py
uv run pytest scripts/tests/test_task_handlers.py
```

## å¼€å‘æŒ‡å—

### æ·»åŠ æ–°ä»»åŠ¡

1. åˆ›å»º TaskHandler:
```python
# scripts/tasks/my_task.py
from scripts.tasks.base import BaseTaskHandler

class MyTaskHandler(BaseTaskHandler):
    @classmethod
    def _get_train_metrics(cls):
        return {"my_reward": "mean"}
    
    def evaluate(self):
        # å®ç°è¯„ä¼°é€»è¾‘
        return {"success": ..., "fail": ...}
    
    def compute_dense_reward(self, info, action):
        # å®ç°å¥–åŠ±è®¡ç®—
        return reward
```

2. åœ¨ `track1_env.py` ä¸­æ³¨å†Œ:
```python
if self.task == "my_task":
    from scripts.tasks.my_task import MyTaskHandler
    return MyTaskHandler(self)
```

### æ·»åŠ æ–° Metrics

åªéœ€åœ¨ TaskHandler çš„ `_get_train_metrics()` æˆ– `_get_eval_metrics()` ä¸­å£°æ˜ï¼š
```python
@classmethod
def _get_train_metrics(cls):
    return {
        "new_metric": "mean",  # æˆ– "sum"
    }
```

ç„¶ååœ¨ `compute_dense_reward()` ä¸­å¡«å……åˆ° `info`:
```python
def compute_dense_reward(self, info, action):
    info["new_metric"] = ...  # è®¡ç®—å€¼
    return reward
```

## å¸¸è§é—®é¢˜

### Q: å¦‚ä½•æŸ¥çœ‹è®­ç»ƒè¿›åº¦ï¼Ÿ
A: ä½¿ç”¨ WandB:
```bash
uv run python scripts/train.py wandb.enabled=true
```

### Q: å¦‚ä½•è°ƒæ•´å¹¶è¡Œç¯å¢ƒæ•°ï¼Ÿ
A: ä¿®æ”¹ `training.num_envs`:
```bash
uv run python scripts/train.py training.num_envs=512
```

### Q: å¦‚ä½•ä¿å­˜/åŠ è½½æ£€æŸ¥ç‚¹ï¼Ÿ
A: æ£€æŸ¥ç‚¹è‡ªåŠ¨ä¿å­˜åˆ° `outputs/YYYY-MM-DD/HH-MM-SS/checkpoints/`ï¼Œä½¿ç”¨ `checkpoint` å‚æ•°åŠ è½½ï¼š
```bash
uv run python scripts/train.py checkpoint=path/to/checkpoint.pth
```

### Q: è¯„ä¼°è§†é¢‘åœ¨å“ªé‡Œï¼Ÿ
A: `outputs/.../videos/` (å®Œæ•´è§†é¢‘) å’Œ `outputs/.../split/evalN/envM/` (åˆ†ç¯å¢ƒè§†é¢‘)

## ç›¸å…³èµ„æº

- **ManiSkill**: https://github.com/haosulab/ManiSkill
- **PPO è®ºæ–‡**: https://arxiv.org/abs/1707.06347
- **é¡¹ç›®æ–‡æ¡£**: `.agent/` ç›®å½•ä¸‹çš„ Markdown æ–‡ä»¶

## License

[Your License Here]

## è´¡çŒ®è€…

[Your Name]

---

æœ€åæ›´æ–°: 2025-12-31